#pragma once

// @generated by aten/src/ATen/gen.py

#include <c10/core/Scalar.h>
#include <ATen/Type.h>
#include <ATen/TypeExtendedInterface.h>
#include <ATen/Tensor.h>
#include <c10/core/Storage.h>
#include <ATen/core/Generator.h>
#include <c10/util/Deprecated.h>
#include <ATen/NativeFunctions.h>
#include <ATen/DeviceGuard.h>
#include <c10/core/TensorOptions.h>
#include <ATen/core/Reduction.h>
#include <c10/util/Optional.h>

namespace at {

using native::from_blob;
using native::tensor;

static inline Tensor & _th_set_(Tensor & self, Storage source);
static inline Tensor & _th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride={});
static inline Tensor & _th_set_(Tensor & self, const Tensor & source);
static inline Tensor & _th_set_(Tensor & self);
static inline Tensor & _th_fill_(Tensor & self, Scalar value);
static inline Tensor & _th_fill_(Tensor & self, const Tensor & value);
static inline Tensor _th_clone(const Tensor & self);
static inline Tensor _th_view(const Tensor & self, IntArrayRef size);
static inline bool _th_equal(const Tensor & self, const Tensor & other);
static inline Tensor & _th_lt_out(Tensor & result, const Tensor & self, Scalar other);
static inline Tensor _th_lt(const Tensor & self, Scalar other);
static inline Tensor & _th_lt_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_lt(const Tensor & self, const Tensor & other);
static inline Tensor & _th_gt_out(Tensor & result, const Tensor & self, Scalar other);
static inline Tensor _th_gt(const Tensor & self, Scalar other);
static inline Tensor & _th_gt_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_gt(const Tensor & self, const Tensor & other);
static inline Tensor & _th_le_out(Tensor & result, const Tensor & self, Scalar other);
static inline Tensor _th_le(const Tensor & self, Scalar other);
static inline Tensor & _th_le_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_le(const Tensor & self, const Tensor & other);
static inline Tensor & _th_ge_out(Tensor & result, const Tensor & self, Scalar other);
static inline Tensor _th_ge(const Tensor & self, Scalar other);
static inline Tensor & _th_ge_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_ge(const Tensor & self, const Tensor & other);
static inline Tensor & _th_eq_out(Tensor & result, const Tensor & self, Scalar other);
static inline Tensor _th_eq(const Tensor & self, Scalar other);
static inline Tensor & _th_eq_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_eq(const Tensor & self, const Tensor & other);
static inline Tensor & _th_ne_out(Tensor & result, const Tensor & self, Scalar other);
static inline Tensor _th_ne(const Tensor & self, Scalar other);
static inline Tensor & _th_ne_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_ne(const Tensor & self, const Tensor & other);
static inline Tensor & _th_min_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_min(const Tensor & self, const Tensor & other);
static inline Tensor _th_min(const Tensor & self);
static inline std::tuple<Tensor &,Tensor &> _th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim=false);
static inline std::tuple<Tensor,Tensor> _th_min(const Tensor & self, int64_t dim, bool keepdim=false);
static inline Tensor & _th_max_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_max(const Tensor & self, const Tensor & other);
static inline Tensor _th_max(const Tensor & self);
static inline std::tuple<Tensor &,Tensor &> _th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim=false);
static inline std::tuple<Tensor,Tensor> _th_max(const Tensor & self, int64_t dim, bool keepdim=false);
static inline Tensor & _th_neg_out(Tensor & result, const Tensor & self);
static inline Tensor _th_neg(const Tensor & self);
static inline Tensor & _th_zero_(Tensor & self);
static inline Tensor & _th_remainder_out(Tensor & result, const Tensor & self, Scalar other);
static inline Tensor _th_remainder(const Tensor & self, Scalar other);
static inline Tensor & _th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other);
static inline Tensor _th_remainder(const Tensor & self, const Tensor & other);
static inline Tensor & _th_remainder_(Tensor & self, Scalar other);
static inline Tensor & _th_remainder_(Tensor & self, const Tensor & other);
static inline Tensor & _th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
static inline Tensor _th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
static inline std::tuple<Tensor &,Tensor &> _thnn_max_pool2d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode);
static inline std::tuple<Tensor,Tensor> _thnn_max_pool2d_with_indices_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode);
static inline std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding);
static inline std::tuple<Tensor,Tensor,Tensor> _thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding);
static inline Tensor add(const Tensor & self, const Tensor & other, Scalar alpha=1);
static inline Tensor add(const Tensor & self, Scalar other, Scalar alpha=1);
static inline Tensor all(const Tensor & self, int64_t dim, bool keepdim=false);
static inline Tensor & all_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim=false);
static inline Tensor as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset=c10::nullopt);
static inline Tensor & as_strided_(Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset=c10::nullopt);
static inline Tensor batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps, bool cudnn_enabled);
static inline std::tuple<Tensor,Tensor,Tensor,int64_t> _batch_norm_impl_index(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps, bool cudnn_enabled);
static inline Tensor _convolution(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled);
static inline Tensor _convolution_nogroup(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding);
static inline Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking=false);
static inline void _copy_same_type_(Tensor & self, const Tensor & src);
static inline Tensor div(const Tensor & self, const Tensor & other);
static inline Tensor div(const Tensor & self, Scalar other);
static inline Tensor empty(IntArrayRef size, const TensorOptions & options={});
static inline Tensor empty_like(const Tensor & self);
static inline Tensor empty_like(const Tensor & self, const TensorOptions & options);
static inline Tensor empty_strided(IntArrayRef size, IntArrayRef stride, const TensorOptions & options={});
static inline Tensor & fill_(Tensor & self, Scalar value);
static inline Tensor & fill_(Tensor & self, const Tensor & value);
static inline Tensor full(IntArrayRef size, Scalar fill_value, const TensorOptions & options={});
static inline bool is_nonzero(const Tensor & self);
static inline Tensor & log_(Tensor & self);
static inline Tensor & log_out(Tensor & out, const Tensor & self);
static inline std::tuple<Tensor,Tensor> max(const Tensor & self, int64_t dim, bool keepdim=false);
static inline std::tuple<Tensor &,Tensor &> max_out(Tensor & max, Tensor & max_values, const Tensor & self, int64_t dim, bool keepdim=false);
static inline Tensor mean(const Tensor & self, ScalarType dtype);
static inline Tensor mean(const Tensor & self);
static inline Tensor mean(const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype);
static inline Tensor mean(const Tensor & self, IntArrayRef dim, bool keepdim=false);
static inline Tensor mean(const Tensor & self, IntArrayRef dim, ScalarType dtype);
static inline std::tuple<Tensor,Tensor> min(const Tensor & self, int64_t dim, bool keepdim=false);
static inline std::tuple<Tensor &,Tensor &> min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim=false);
static inline Tensor mul(const Tensor & self, const Tensor & other);
static inline Tensor mul(const Tensor & self, Scalar other);
static inline Tensor narrow(const Tensor & self, int64_t dim, int64_t start, int64_t length);
static inline std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps);
static inline Tensor ones(IntArrayRef size, const TensorOptions & options={});
static inline Tensor scalar_tensor(Scalar s, const TensorOptions & options={});
static inline Tensor select(const Tensor & self, int64_t dim, int64_t index);
static inline int64_t size(const Tensor & self, int64_t dim);
static inline Tensor slice(const Tensor & self, int64_t dim=0, int64_t start=0, int64_t end=9223372036854775807, int64_t step=1);
static inline Tensor squeeze(const Tensor & self);
static inline Tensor squeeze(const Tensor & self, int64_t dim);
static inline int64_t stride(const Tensor & self, int64_t dim);
static inline Tensor sum(const Tensor & self, ScalarType dtype);
static inline Tensor sum(const Tensor & self);
static inline Tensor sum(const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype);
static inline Tensor sum(const Tensor & self, IntArrayRef dim, bool keepdim=false);
static inline Tensor sum(const Tensor & self, IntArrayRef dim, ScalarType dtype);
static inline Tensor & sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype);
static inline Tensor & sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim=false);
static inline Tensor & sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, ScalarType dtype);
static inline Tensor t(const Tensor & self);
static inline Tensor & threshold_(Tensor & self, Scalar threshold, Scalar value);
static inline Tensor transpose(const Tensor & self, int64_t dim0, int64_t dim1);
static inline Tensor unsqueeze(const Tensor & self, int64_t dim);
static inline Tensor zeros(IntArrayRef size, const TensorOptions & options={});
static inline Tensor native_clone(const Tensor & self);
static inline Tensor clone(const Tensor & self);
static inline Tensor & native_zero_(Tensor & self);
static inline Tensor & zero_(Tensor & self);
static inline Tensor sub(const Tensor & self, const Tensor & other, Scalar alpha=1);
static inline Tensor sub(const Tensor & self, Scalar other, Scalar alpha=1);
static inline Tensor addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
static inline int64_t numel(const Tensor & self);
static inline Scalar _local_scalar_dense(const Tensor & self);
static inline Tensor ne(const Tensor & self, Scalar other);
static inline Tensor ne(const Tensor & self, const Tensor & other);
static inline Tensor eq(const Tensor & self, Scalar other);
static inline Tensor eq(const Tensor & self, const Tensor & other);
static inline Tensor ge(const Tensor & self, Scalar other);
static inline Tensor ge(const Tensor & self, const Tensor & other);
static inline Tensor le(const Tensor & self, Scalar other);
static inline Tensor le(const Tensor & self, const Tensor & other);
static inline Tensor gt(const Tensor & self, Scalar other);
static inline Tensor gt(const Tensor & self, const Tensor & other);
static inline Tensor lt(const Tensor & self, Scalar other);
static inline Tensor lt(const Tensor & self, const Tensor & other);
static inline Tensor neg(const Tensor & self);
static inline Tensor remainder(const Tensor & self, Scalar other);
static inline Tensor remainder(const Tensor & self, const Tensor & other);
static inline Tensor & min_out(Tensor & out, const Tensor & self, const Tensor & other);
static inline Tensor min(const Tensor & self, const Tensor & other);
static inline Tensor min(const Tensor & self);
static inline Tensor & max_out(Tensor & out, const Tensor & self, const Tensor & other);
static inline Tensor max(const Tensor & self, const Tensor & other);
static inline Tensor max(const Tensor & self);
static inline Tensor all(const Tensor & self);
static inline bool equal(const Tensor & self, const Tensor & other);
static inline Tensor adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size);
static inline Tensor _adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size);
static inline std::tuple<Tensor,Tensor> max_pool2d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride={}, IntArrayRef padding=0, IntArrayRef dilation=1, bool ceil_mode=false);
static inline Tensor thnn_conv2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias={}, IntArrayRef stride=1, IntArrayRef padding=0);
static inline std::tuple<Tensor,Tensor,Tensor> thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding);

namespace detail {

static inline TypeExtendedInterface & infer_type(const Tensor & t) {
  AT_CHECK(t.defined(), "undefined Tensor");
  return getType(t);
}
static inline TypeExtendedInterface & infer_type(const TensorList & tl) {
  AT_CHECK(tl.size() > 0, "expected a non-empty list of Tensors");
  return getType(tl[0]);
}

} // namespace detail

// function definitions are all static inline because
// they are one-line statically dispatched functions that
// invoke the actual dynamic dispatch on the correct argument
static inline Tensor & _th_set_(Tensor & self, Storage source) {
    return detail::infer_type(self)._th_set_(self, source);
}
static inline Tensor & _th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) {
    return detail::infer_type(self)._th_set_(self, source, storage_offset, size, stride);
}
static inline Tensor & _th_set_(Tensor & self, const Tensor & source) {
    return detail::infer_type(self)._th_set_(self, source);
}
static inline Tensor & _th_set_(Tensor & self) {
    return detail::infer_type(self)._th_set_(self);
}
static inline Tensor & _th_fill_(Tensor & self, Scalar value) {
    return detail::infer_type(self)._th_fill_(self, value);
}
static inline Tensor & _th_fill_(Tensor & self, const Tensor & value) {
    return detail::infer_type(self)._th_fill_(self, value);
}
static inline Tensor _th_clone(const Tensor & self) {
    return detail::infer_type(self)._th_clone(self);
}
static inline Tensor _th_view(const Tensor & self, IntArrayRef size) {
    return detail::infer_type(self)._th_view(self, size);
}
static inline bool _th_equal(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_equal(self, other);
}
static inline Tensor & _th_lt_out(Tensor & result, const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_lt_out(result, self, other);
}
static inline Tensor _th_lt(const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_lt(self, other);
}
static inline Tensor & _th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_lt_out(result, self, other);
}
static inline Tensor _th_lt(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_lt(self, other);
}
static inline Tensor & _th_gt_out(Tensor & result, const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_gt_out(result, self, other);
}
static inline Tensor _th_gt(const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_gt(self, other);
}
static inline Tensor & _th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_gt_out(result, self, other);
}
static inline Tensor _th_gt(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_gt(self, other);
}
static inline Tensor & _th_le_out(Tensor & result, const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_le_out(result, self, other);
}
static inline Tensor _th_le(const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_le(self, other);
}
static inline Tensor & _th_le_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_le_out(result, self, other);
}
static inline Tensor _th_le(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_le(self, other);
}
static inline Tensor & _th_ge_out(Tensor & result, const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_ge_out(result, self, other);
}
static inline Tensor _th_ge(const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_ge(self, other);
}
static inline Tensor & _th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_ge_out(result, self, other);
}
static inline Tensor _th_ge(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_ge(self, other);
}
static inline Tensor & _th_eq_out(Tensor & result, const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_eq_out(result, self, other);
}
static inline Tensor _th_eq(const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_eq(self, other);
}
static inline Tensor & _th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_eq_out(result, self, other);
}
static inline Tensor _th_eq(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_eq(self, other);
}
static inline Tensor & _th_ne_out(Tensor & result, const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_ne_out(result, self, other);
}
static inline Tensor _th_ne(const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_ne(self, other);
}
static inline Tensor & _th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_ne_out(result, self, other);
}
static inline Tensor _th_ne(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_ne(self, other);
}
static inline Tensor & _th_min_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_min_out(result, self, other);
}
static inline Tensor _th_min(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_min(self, other);
}
static inline Tensor _th_min(const Tensor & self) {
    return detail::infer_type(self)._th_min(self);
}
static inline std::tuple<Tensor &,Tensor &> _th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self)._th_min_out(min, min_indices, self, dim, keepdim);
}
static inline std::tuple<Tensor,Tensor> _th_min(const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self)._th_min(self, dim, keepdim);
}
static inline Tensor & _th_max_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_max_out(result, self, other);
}
static inline Tensor _th_max(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_max(self, other);
}
static inline Tensor _th_max(const Tensor & self) {
    return detail::infer_type(self)._th_max(self);
}
static inline std::tuple<Tensor &,Tensor &> _th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self)._th_max_out(max, max_indices, self, dim, keepdim);
}
static inline std::tuple<Tensor,Tensor> _th_max(const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self)._th_max(self, dim, keepdim);
}
static inline Tensor & _th_neg_out(Tensor & result, const Tensor & self) {
    return detail::infer_type(self)._th_neg_out(result, self);
}
static inline Tensor _th_neg(const Tensor & self) {
    return detail::infer_type(self)._th_neg(self);
}
static inline Tensor & _th_zero_(Tensor & self) {
    return detail::infer_type(self)._th_zero_(self);
}
static inline Tensor & _th_remainder_out(Tensor & result, const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_remainder_out(result, self, other);
}
static inline Tensor _th_remainder(const Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_remainder(self, other);
}
static inline Tensor & _th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_remainder_out(result, self, other);
}
static inline Tensor _th_remainder(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_remainder(self, other);
}
static inline Tensor & _th_remainder_(Tensor & self, Scalar other) {
    return detail::infer_type(self)._th_remainder_(self, other);
}
static inline Tensor & _th_remainder_(Tensor & self, const Tensor & other) {
    return detail::infer_type(self)._th_remainder_(self, other);
}
static inline Tensor & _th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
    return detail::infer_type(self)._th_addmm_out(result, self, mat1, mat2, beta, alpha);
}
static inline Tensor _th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
    return detail::infer_type(self)._th_addmm(self, mat1, mat2, beta, alpha);
}
static inline std::tuple<Tensor &,Tensor &> _thnn_max_pool2d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
    return detail::infer_type(self)._thnn_max_pool2d_with_indices_forward_out(output, indices, self, kernel_size, stride, padding, dilation, ceil_mode);
}
static inline std::tuple<Tensor,Tensor> _thnn_max_pool2d_with_indices_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
    return detail::infer_type(self)._thnn_max_pool2d_with_indices_forward(self, kernel_size, stride, padding, dilation, ceil_mode);
}
static inline std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) {
    return detail::infer_type(self)._thnn_conv2d_forward_out(output, finput, fgrad_input, self, weight, kernel_size, bias, stride, padding);
}
static inline std::tuple<Tensor,Tensor,Tensor> _thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) {
    return detail::infer_type(self)._thnn_conv2d_forward(self, weight, kernel_size, bias, stride, padding);
}
static inline Tensor add(const Tensor & self, const Tensor & other, Scalar alpha) {
    return detail::infer_type(self).add(self, other, alpha);
}
static inline Tensor add(const Tensor & self, Scalar other, Scalar alpha) {
    return detail::infer_type(self).add(self, other, alpha);
}
static inline Tensor all(const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self).all(self, dim, keepdim);
}
static inline Tensor & all_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self).all_out(out, self, dim, keepdim);
}
static inline Tensor as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    return detail::infer_type(self).as_strided(self, size, stride, storage_offset);
}
static inline Tensor & as_strided_(Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    return detail::infer_type(self).as_strided_(self, size, stride, storage_offset);
}
static inline Tensor batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    return detail::infer_type(input).batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
static inline std::tuple<Tensor,Tensor,Tensor,int64_t> _batch_norm_impl_index(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    return detail::infer_type(input)._batch_norm_impl_index(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
static inline Tensor _convolution(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
    return detail::infer_type(input)._convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}
static inline Tensor _convolution_nogroup(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding) {
    return detail::infer_type(input)._convolution_nogroup(input, weight, bias, stride, padding, dilation, transposed, output_padding);
}
static inline Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) {
    return detail::infer_type(self).s_copy_(self, src, non_blocking);
}
static inline void _copy_same_type_(Tensor & self, const Tensor & src) {
    return detail::infer_type(self)._copy_same_type_(self, src);
}
static inline Tensor div(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).div(self, other);
}
static inline Tensor div(const Tensor & self, Scalar other) {
    return detail::infer_type(self).div(self, other);
}
static inline Tensor empty(IntArrayRef size, const TensorOptions & options) {
    return at::getType(options).empty(size, options);
}
static inline Tensor empty_like(const Tensor & self) {
    return detail::infer_type(self).empty_like(self);
}
static inline Tensor empty_like(const Tensor & self, const TensorOptions & options) {
    return at::getType(options).empty_like(self, options);
}
static inline Tensor empty_strided(IntArrayRef size, IntArrayRef stride, const TensorOptions & options) {
    return at::getType(options).empty_strided(size, stride, options);
}
static inline Tensor & fill_(Tensor & self, Scalar value) {
    return detail::infer_type(self).fill_(self, value);
}
static inline Tensor & fill_(Tensor & self, const Tensor & value) {
    return detail::infer_type(self).fill_(self, value);
}
static inline Tensor full(IntArrayRef size, Scalar fill_value, const TensorOptions & options) {
    return at::getType(options).full(size, fill_value, options);
}
static inline bool is_nonzero(const Tensor & self) {
    return detail::infer_type(self).is_nonzero(self);
}
static inline Tensor & log_(Tensor & self) {
    return detail::infer_type(self).log_(self);
}
static inline Tensor & log_out(Tensor & out, const Tensor & self) {
    return detail::infer_type(self).log_out(out, self);
}
static inline std::tuple<Tensor,Tensor> max(const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self).max(self, dim, keepdim);
}
static inline std::tuple<Tensor &,Tensor &> max_out(Tensor & max, Tensor & max_values, const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self).max_out(max, max_values, self, dim, keepdim);
}
static inline Tensor mean(const Tensor & self, ScalarType dtype) {
    return detail::infer_type(self).mean(self, dtype);
}
static inline Tensor mean(const Tensor & self) {
    return detail::infer_type(self).mean(self);
}
static inline Tensor mean(const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype) {
    return detail::infer_type(self).mean(self, dim, keepdim, dtype);
}
static inline Tensor mean(const Tensor & self, IntArrayRef dim, bool keepdim) {
    return detail::infer_type(self).mean(self, dim, keepdim);
}
static inline Tensor mean(const Tensor & self, IntArrayRef dim, ScalarType dtype) {
    return detail::infer_type(self).mean(self, dim, dtype);
}
static inline std::tuple<Tensor,Tensor> min(const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self).min(self, dim, keepdim);
}
static inline std::tuple<Tensor &,Tensor &> min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) {
    return detail::infer_type(self).min_out(min, min_indices, self, dim, keepdim);
}
static inline Tensor mul(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).mul(self, other);
}
static inline Tensor mul(const Tensor & self, Scalar other) {
    return detail::infer_type(self).mul(self, other);
}
static inline Tensor narrow(const Tensor & self, int64_t dim, int64_t start, int64_t length) {
    return detail::infer_type(self).narrow(self, dim, start, length);
}
static inline std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) {
    return detail::infer_type(input).native_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
static inline Tensor ones(IntArrayRef size, const TensorOptions & options) {
    return at::getType(options).ones(size, options);
}
static inline Tensor scalar_tensor(Scalar s, const TensorOptions & options) {
    return at::getType(options).scalar_tensor(s, options);
}
static inline Tensor select(const Tensor & self, int64_t dim, int64_t index) {
    return detail::infer_type(self).select(self, dim, index);
}
static inline int64_t size(const Tensor & self, int64_t dim) {
    return detail::infer_type(self).size(self, dim);
}
static inline Tensor slice(const Tensor & self, int64_t dim, int64_t start, int64_t end, int64_t step) {
    return detail::infer_type(self).slice(self, dim, start, end, step);
}
static inline Tensor squeeze(const Tensor & self) {
    return detail::infer_type(self).squeeze(self);
}
static inline Tensor squeeze(const Tensor & self, int64_t dim) {
    return detail::infer_type(self).squeeze(self, dim);
}
static inline int64_t stride(const Tensor & self, int64_t dim) {
    return detail::infer_type(self).stride(self, dim);
}
static inline Tensor sum(const Tensor & self, ScalarType dtype) {
    return detail::infer_type(self).sum(self, dtype);
}
static inline Tensor sum(const Tensor & self) {
    return detail::infer_type(self).sum(self);
}
static inline Tensor sum(const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype) {
    return detail::infer_type(self).sum(self, dim, keepdim, dtype);
}
static inline Tensor sum(const Tensor & self, IntArrayRef dim, bool keepdim) {
    return detail::infer_type(self).sum(self, dim, keepdim);
}
static inline Tensor sum(const Tensor & self, IntArrayRef dim, ScalarType dtype) {
    return detail::infer_type(self).sum(self, dim, dtype);
}
static inline Tensor & sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype) {
    return detail::infer_type(self).sum_out(out, self, dim, keepdim, dtype);
}
static inline Tensor & sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim) {
    return detail::infer_type(self).sum_out(out, self, dim, keepdim);
}
static inline Tensor & sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, ScalarType dtype) {
    return detail::infer_type(self).sum_out(out, self, dim, dtype);
}
static inline Tensor t(const Tensor & self) {
    return detail::infer_type(self).t(self);
}
static inline Tensor & threshold_(Tensor & self, Scalar threshold, Scalar value) {
    return detail::infer_type(self).threshold_(self, threshold, value);
}
static inline Tensor transpose(const Tensor & self, int64_t dim0, int64_t dim1) {
    return detail::infer_type(self).transpose(self, dim0, dim1);
}
static inline Tensor unsqueeze(const Tensor & self, int64_t dim) {
    return detail::infer_type(self).unsqueeze(self, dim);
}
static inline Tensor zeros(IntArrayRef size, const TensorOptions & options) {
    return at::getType(options).zeros(size, options);
}
static inline Tensor native_clone(const Tensor & self) {
    return detail::infer_type(self).native_clone(self);
}
static inline Tensor clone(const Tensor & self) {
    return detail::infer_type(self).clone(self);
}
static inline Tensor & native_zero_(Tensor & self) {
    return detail::infer_type(self).native_zero_(self);
}
static inline Tensor & zero_(Tensor & self) {
    return detail::infer_type(self).zero_(self);
}
static inline Tensor sub(const Tensor & self, const Tensor & other, Scalar alpha) {
    return detail::infer_type(self).sub(self, other, alpha);
}
static inline Tensor sub(const Tensor & self, Scalar other, Scalar alpha) {
    return detail::infer_type(self).sub(self, other, alpha);
}
static inline Tensor addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
    return detail::infer_type(self).addmm(self, mat1, mat2, beta, alpha);
}
static inline int64_t numel(const Tensor & self) {
    return detail::infer_type(self).numel(self);
}
static inline Scalar _local_scalar_dense(const Tensor & self) {
    return detail::infer_type(self)._local_scalar_dense(self);
}
static inline Tensor ne(const Tensor & self, Scalar other) {
    return detail::infer_type(self).ne(self, other);
}
static inline Tensor ne(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).ne(self, other);
}
static inline Tensor eq(const Tensor & self, Scalar other) {
    return detail::infer_type(self).eq(self, other);
}
static inline Tensor eq(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).eq(self, other);
}
static inline Tensor ge(const Tensor & self, Scalar other) {
    return detail::infer_type(self).ge(self, other);
}
static inline Tensor ge(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).ge(self, other);
}
static inline Tensor le(const Tensor & self, Scalar other) {
    return detail::infer_type(self).le(self, other);
}
static inline Tensor le(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).le(self, other);
}
static inline Tensor gt(const Tensor & self, Scalar other) {
    return detail::infer_type(self).gt(self, other);
}
static inline Tensor gt(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).gt(self, other);
}
static inline Tensor lt(const Tensor & self, Scalar other) {
    return detail::infer_type(self).lt(self, other);
}
static inline Tensor lt(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).lt(self, other);
}
static inline Tensor neg(const Tensor & self) {
    return detail::infer_type(self).neg(self);
}
static inline Tensor remainder(const Tensor & self, Scalar other) {
    return detail::infer_type(self).remainder(self, other);
}
static inline Tensor remainder(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).remainder(self, other);
}
static inline Tensor & min_out(Tensor & out, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).min_out(out, self, other);
}
static inline Tensor min(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).min(self, other);
}
static inline Tensor min(const Tensor & self) {
    return detail::infer_type(self).min(self);
}
static inline Tensor & max_out(Tensor & out, const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).max_out(out, self, other);
}
static inline Tensor max(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).max(self, other);
}
static inline Tensor max(const Tensor & self) {
    return detail::infer_type(self).max(self);
}
static inline Tensor all(const Tensor & self) {
    return detail::infer_type(self).all(self);
}
static inline bool equal(const Tensor & self, const Tensor & other) {
    return detail::infer_type(self).equal(self, other);
}
static inline Tensor adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) {
    return detail::infer_type(self).adaptive_avg_pool2d(self, output_size);
}
static inline Tensor _adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) {
    return detail::infer_type(self)._adaptive_avg_pool2d(self, output_size);
}
static inline std::tuple<Tensor,Tensor> max_pool2d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
    return detail::infer_type(self).max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}
static inline Tensor thnn_conv2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) {
    return detail::infer_type(self).thnn_conv2d(self, weight, kernel_size, bias, stride, padding);
}
static inline std::tuple<Tensor,Tensor,Tensor> thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) {
    return detail::infer_type(self).thnn_conv2d_forward(self, weight, kernel_size, bias, stride, padding);
}

}
